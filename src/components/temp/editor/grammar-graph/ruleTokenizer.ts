import { Lexer, Parser } from 'antlr4ts';
import { Log } from '../../../utility/log';
import { VocabularyPack } from '../vocabularyPack';
import { RuleToken } from './ruleToken';
import { TokenizerContext } from './tokenizerContext';
import { TokenType } from './tokenType';

export const rgxEscape = /[-/\\^$*+?.()|[\]{}]/g;
export const rgxReplace = '\\$&';

export class RuleTokenizer<L extends Lexer, P extends Parser> {
	private vocabulary: VocabularyPack<L, P>;
	private readonly context: TokenizerContext = {
		tokens: [],
		parens: [],
		atom: [],
		modifier: undefined,
	};

	constructor(vocabulary: VocabularyPack<L, P>) {
		this.vocabulary = vocabulary;
	}

	public tokenize(declaration: string): RuleToken[] {
		this.clearContext();
		Array.from(declaration).forEach((value, index) => {
			if (!(this.isAtom(value, index) || this.isModifier(value, index))) {
				switch (value) {
					case TokenType.Lparen: {
						const token = new RuleToken(
							TokenType.Lparen,
							this.context.parens.length
						);
						this.context.parens.push(token);
						this.context.tokens.push(token);
						break;
					}
					case TokenType.Rparen: {
						const lparen = this.context.parens.pop();
						if (lparen) {
							const lindex = this.context.tokens.indexOf(lparen);
							if (lindex !== this.context.tokens.length - 2) {
								const rparen = new RuleToken(
									TokenType.Rparen,
									this.context.parens.length
								);
								lparen.sibling = rparen;
								rparen.sibling = lparen;
								this.context.tokens.push(rparen);
							} else {
								const last = this.lastToken();
								if (last) last.nested--;
								this.context.tokens.splice(lindex, 1);
							}
						} else this.unexpected(value, index);
						break;
					}
					case TokenType.Pipe: {
						this.addToken(value);
						break;
					}
					case TokenType.Assign: {
						const last = this.lastToken([TokenType.Unknown]);
						if (last) {
							last.type = TokenType.Identifier;
							this.addToken(value);
							Log.warn(
								'Unknown operator token ' +
									last.name +
									' recognized as identifier',
								'RuleTokenizer'
							);
						}
						break;
					}
					case TokenType.Hash: {
						this.addToken(value);
						break;
					}
					case TokenType.Space: {
						this.finishAtom(index);
						break;
					}
					default: {
						this.unexpected(value, index);
					}
				}
			}
		});
		this.finishAtom(declaration.length);
		return this.context.tokens;
	}

	private isAtom(value: string, index: number): boolean {
		if (/[a-zA-Z_0-9]/.test(value)) {
			this.context.atom.push(value);
			return true;
		}
		this.finishAtom(index);
		return false;
	}

	private isModifier(value: string, index: number): boolean {
		// TODO: add plus assign and multipliers stacking
		if (/[*+?]/.test(value)) {
			const last = this.lastToken([
				TokenType.Keyword,
				TokenType.Operator,
				TokenType.Operand,
				TokenType.Rule,
				TokenType.Rparen,
				TokenType.Unknown,
			]);
			if (this.context.modifier) {
				if (value === TokenType.Question && last) last.greedy = false;
				else this.unexpected(value, index);
				this.finishModifier();
			} else {
				if (last) {
					last.multiply(value);
					this.context.modifier = value as any;
				} else this.unexpected(value, index);
			}
			return true;
		}
		this.finishModifier();
		return false;
	}

	private addToken(type: TokenType) {
		this.context.tokens.push(new RuleToken(type, this.context.parens.length));
	}

	private finishAtom(index: number) {
		if (this.context.atom.length > 0) {
			const name: string = this.context.atom.join('');
			let value: string | undefined = undefined;
			index = index - this.context.atom.length;
			const last = this.lastToken([TokenType.Hash]);
			let type: TokenType;
			if (last) {
				type = TokenType.Label;
			} else if (this.vocabulary.isSymbolicName(name)) {
				type = TokenType.Operand;
				const symbolicIndex = this.vocabulary.symbolicIndex(name);
				if (this.vocabulary.hasKeyword(symbolicIndex)) {
					type = TokenType.Keyword;
					value = this.vocabulary.keyword(symbolicIndex);
				} else if (this.vocabulary.hasOperator(symbolicIndex)) {
					type = TokenType.Operator;
					value = this.vocabulary.operator(symbolicIndex);
				}
			} else if (name in Lexer) {
				type = TokenType.Operator;
			} else if (this.vocabulary.isRuleName(name)) {
				type = TokenType.Rule;
			} else {
				type = TokenType.Unknown;
				Log.warn(
					'Unknown operator token ' + name + ' at ' + index,
					'RuleTokenizer'
				);
			}

			this.context.tokens.push(
				new RuleToken(type, this.context.parens.length, name, value)
			);
			this.context.atom.length = 0;
		}
	}

	private finishModifier() {
		this.context.modifier = undefined;
	}

	private unexpected(token: string, index: number) {
		Log.error('Unexpected token ' + token + ' at ' + index, 'RuleTokenizer');
	}

	private clearContext() {
		this.context.tokens = [];
		this.context.parens = [];
		this.context.atom = [];
		this.context.modifier = undefined;
	}

	private lastToken(filter?: TokenType[]): RuleToken | undefined {
		if (this.context.tokens.length > 0) {
			const last = this.context.tokens[this.context.tokens.length - 1];
			if (last && (!filter || filter.includes(last.type))) return last;
		}
		return undefined;
	}

	public ruleName(declaration: string, index: number): string {
		const ruleNames = this.vocabulary.getRuleNames();
		let rgx = new RegExp(ruleNames[index].replace(rgxEscape, rgxReplace), 'g');
		if (rgx.test(declaration)) {
			return ruleNames[index];
		} else {
			Log.warn(
				'Mismatched rule ' + index + ' name. Looking for alternatives.',
				'RuleTokenizer'
			);
			ruleNames.forEach((ruleName, other) => {
				rgx = new RegExp(ruleName.replace(rgxEscape, rgxReplace), 'g');
				if (rgx.test(declaration)) {
					Log.warn('Matched name of rule ' + other + '.', 'RuleTokenizer');
					return ruleName;
				}
			});
			Log.error('No alternative for mismatched rule name.', 'RuleTokenizer');
		}
		return declaration;
	}

	public static alternatives(tokens: RuleToken[]): RuleToken[][] {
		let pipeIndex = -1;
		let statement: RuleToken[] = [];
		return tokens.reduce((statements: RuleToken[][], token, index, initial) => {
			if (token.nested === 0 && token.type === TokenType.Pipe) {
				statement = initial.slice(pipeIndex + 1, index);
				if (statement.length !== 0) statements.push(statement);
				pipeIndex = index;
			} else if (index === initial.length - 1) {
				statement = initial.slice(pipeIndex + 1);
				if (statement.length !== 0) statements.push(statement);
			}
			return statements;
		}, []);
	}

	public static parentheses(tokens: RuleToken[]): RuleToken[][] {
		let parenIndex = -1;
		let statement: RuleToken[] = [];
		return tokens.reduce((statements: RuleToken[][], token, index, initial) => {
			if (
				token.nested === 0 &&
				(token.type === TokenType.Lparen || token.type === TokenType.Rparen)
			) {
				statement = initial.slice(
					parenIndex,
					index + (token.type === TokenType.Rparen ? 1 : 0)
				);
				if (statement.length !== 0) statements.push(statement);
				parenIndex = index;
			} else if (index === initial.length - 1) {
				statement = initial.slice(parenIndex + 1);
				if (statement.length !== 0) statements.push(statement);
			}
			return statements;
		}, []);
	}

	static unnest(tokens: RuleToken[]) {
		if (!tokens) return [];
		const left = tokens[0];
		const right = tokens[tokens.length - 1];
		if (
			left.type === TokenType.Lparen &&
			right.type === TokenType.Rparen &&
			left.sibling === right
		) {
			tokens.pop();
			tokens.shift();
			tokens.forEach((token) => token.nested--);
		}
		return tokens;
	}
}
